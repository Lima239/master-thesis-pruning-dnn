{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3c4d2bf4-a5a5-496c-8c30-6061803931ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1608b892",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# Tu si treba vybrat jedno GPU (cize bud 0 alebo 1)\n",
    "# Je to cislovane naopak ako v nvidia-smi, because ..., that's why\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\" \n",
    "os.environ[\"OMP_NUM_THREADS\"] = \"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8b12ab01",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import os\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets\n",
    "import time\n",
    "import copy\n",
    "import sys\n",
    "\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.init as init\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "\n",
    "import sys\n",
    "import numpy as np\n",
    "import timm\n",
    "from functools import partial\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "84193af4-3f22-4cb1-b7b3-0b93daeef9d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: jupyter in /home/jlichmanova/master-thesis-pruning-dnn/mip-env/lib/python3.10/site-packages (1.1.1)\n",
      "Requirement already satisfied: ipywidgets in /home/jlichmanova/master-thesis-pruning-dnn/mip-env/lib/python3.10/site-packages (8.1.5)\n",
      "Requirement already satisfied: ipykernel in /home/jlichmanova/master-thesis-pruning-dnn/mip-env/lib/python3.10/site-packages (from jupyter) (6.29.5)\n",
      "Requirement already satisfied: jupyter-console in /home/jlichmanova/master-thesis-pruning-dnn/mip-env/lib/python3.10/site-packages (from jupyter) (6.6.3)\n",
      "Requirement already satisfied: notebook in /home/jlichmanova/master-thesis-pruning-dnn/mip-env/lib/python3.10/site-packages (from jupyter) (7.2.2)\n",
      "Requirement already satisfied: nbconvert in /home/jlichmanova/master-thesis-pruning-dnn/mip-env/lib/python3.10/site-packages (from jupyter) (7.16.4)\n",
      "Requirement already satisfied: jupyterlab in /home/jlichmanova/master-thesis-pruning-dnn/mip-env/lib/python3.10/site-packages (from jupyter) (4.2.6)\n",
      "Requirement already satisfied: ipython>=6.1.0 in /home/jlichmanova/master-thesis-pruning-dnn/mip-env/lib/python3.10/site-packages (from ipywidgets) (8.30.0)\n",
      "Requirement already satisfied: jupyterlab-widgets~=3.0.12 in /home/jlichmanova/master-thesis-pruning-dnn/mip-env/lib/python3.10/site-packages (from ipywidgets) (3.0.13)\n",
      "Requirement already satisfied: traitlets>=4.3.1 in /home/jlichmanova/master-thesis-pruning-dnn/mip-env/lib/python3.10/site-packages (from ipywidgets) (5.14.3)\n",
      "Requirement already satisfied: comm>=0.1.3 in /home/jlichmanova/master-thesis-pruning-dnn/mip-env/lib/python3.10/site-packages (from ipywidgets) (0.2.2)\n",
      "Requirement already satisfied: widgetsnbextension~=4.0.12 in /home/jlichmanova/master-thesis-pruning-dnn/mip-env/lib/python3.10/site-packages (from ipywidgets) (4.0.13)\n",
      "Requirement already satisfied: stack_data in /home/jlichmanova/master-thesis-pruning-dnn/mip-env/lib/python3.10/site-packages (from ipython>=6.1.0->ipywidgets) (0.6.3)\n",
      "Requirement already satisfied: jedi>=0.16 in /home/jlichmanova/master-thesis-pruning-dnn/mip-env/lib/python3.10/site-packages (from ipython>=6.1.0->ipywidgets) (0.19.2)\n",
      "Requirement already satisfied: matplotlib-inline in /home/jlichmanova/master-thesis-pruning-dnn/mip-env/lib/python3.10/site-packages (from ipython>=6.1.0->ipywidgets) (0.1.7)\n",
      "Requirement already satisfied: exceptiongroup in /home/jlichmanova/master-thesis-pruning-dnn/mip-env/lib/python3.10/site-packages (from ipython>=6.1.0->ipywidgets) (1.2.2)\n",
      "Requirement already satisfied: typing_extensions>=4.6 in /home/jlichmanova/master-thesis-pruning-dnn/mip-env/lib/python3.10/site-packages (from ipython>=6.1.0->ipywidgets) (4.12.2)\n",
      "Requirement already satisfied: pygments>=2.4.0 in /home/jlichmanova/master-thesis-pruning-dnn/mip-env/lib/python3.10/site-packages (from ipython>=6.1.0->ipywidgets) (2.18.0)\n",
      "Requirement already satisfied: decorator in /home/jlichmanova/master-thesis-pruning-dnn/mip-env/lib/python3.10/site-packages (from ipython>=6.1.0->ipywidgets) (5.1.1)\n",
      "Requirement already satisfied: prompt_toolkit<3.1.0,>=3.0.41 in /home/jlichmanova/master-thesis-pruning-dnn/mip-env/lib/python3.10/site-packages (from ipython>=6.1.0->ipywidgets) (3.0.48)\n",
      "Requirement already satisfied: pexpect>4.3 in /home/jlichmanova/master-thesis-pruning-dnn/mip-env/lib/python3.10/site-packages (from ipython>=6.1.0->ipywidgets) (4.9.0)\n",
      "Requirement already satisfied: debugpy>=1.6.5 in /home/jlichmanova/master-thesis-pruning-dnn/mip-env/lib/python3.10/site-packages (from ipykernel->jupyter) (1.8.9)\n",
      "Requirement already satisfied: psutil in /home/jlichmanova/master-thesis-pruning-dnn/mip-env/lib/python3.10/site-packages (from ipykernel->jupyter) (6.1.0)\n",
      "Requirement already satisfied: tornado>=6.1 in /home/jlichmanova/master-thesis-pruning-dnn/mip-env/lib/python3.10/site-packages (from ipykernel->jupyter) (6.4.2)\n",
      "Requirement already satisfied: nest-asyncio in /home/jlichmanova/master-thesis-pruning-dnn/mip-env/lib/python3.10/site-packages (from ipykernel->jupyter) (1.6.0)\n",
      "Requirement already satisfied: jupyter-client>=6.1.12 in /home/jlichmanova/master-thesis-pruning-dnn/mip-env/lib/python3.10/site-packages (from ipykernel->jupyter) (8.6.3)\n",
      "Requirement already satisfied: packaging in /home/jlichmanova/master-thesis-pruning-dnn/mip-env/lib/python3.10/site-packages (from ipykernel->jupyter) (24.2)\n",
      "Requirement already satisfied: pyzmq>=24 in /home/jlichmanova/master-thesis-pruning-dnn/mip-env/lib/python3.10/site-packages (from ipykernel->jupyter) (26.2.0)\n",
      "Requirement already satisfied: jupyter-core!=5.0.*,>=4.12 in /home/jlichmanova/master-thesis-pruning-dnn/mip-env/lib/python3.10/site-packages (from ipykernel->jupyter) (5.7.2)\n",
      "Requirement already satisfied: async-lru>=1.0.0 in /home/jlichmanova/master-thesis-pruning-dnn/mip-env/lib/python3.10/site-packages (from jupyterlab->jupyter) (2.0.4)\n",
      "Requirement already satisfied: httpx>=0.25.0 in /home/jlichmanova/master-thesis-pruning-dnn/mip-env/lib/python3.10/site-packages (from jupyterlab->jupyter) (0.28.0)\n",
      "Requirement already satisfied: jupyterlab-server<3,>=2.27.1 in /home/jlichmanova/master-thesis-pruning-dnn/mip-env/lib/python3.10/site-packages (from jupyterlab->jupyter) (2.27.3)\n",
      "Requirement already satisfied: tomli>=1.2.2 in /home/jlichmanova/master-thesis-pruning-dnn/mip-env/lib/python3.10/site-packages (from jupyterlab->jupyter) (2.2.1)\n",
      "Requirement already satisfied: jinja2>=3.0.3 in /home/jlichmanova/master-thesis-pruning-dnn/mip-env/lib/python3.10/site-packages (from jupyterlab->jupyter) (3.1.4)\n",
      "Requirement already satisfied: jupyter-lsp>=2.0.0 in /home/jlichmanova/master-thesis-pruning-dnn/mip-env/lib/python3.10/site-packages (from jupyterlab->jupyter) (2.2.5)\n",
      "Requirement already satisfied: notebook-shim>=0.2 in /home/jlichmanova/master-thesis-pruning-dnn/mip-env/lib/python3.10/site-packages (from jupyterlab->jupyter) (0.2.4)\n",
      "Requirement already satisfied: jupyter-server<3,>=2.4.0 in /home/jlichmanova/master-thesis-pruning-dnn/mip-env/lib/python3.10/site-packages (from jupyterlab->jupyter) (2.14.2)\n",
      "Requirement already satisfied: setuptools>=40.1.0 in /home/jlichmanova/master-thesis-pruning-dnn/mip-env/lib/python3.10/site-packages (from jupyterlab->jupyter) (59.6.0)\n",
      "Requirement already satisfied: markupsafe>=2.0 in /home/jlichmanova/master-thesis-pruning-dnn/mip-env/lib/python3.10/site-packages (from nbconvert->jupyter) (3.0.2)\n",
      "Requirement already satisfied: mistune<4,>=2.0.3 in /home/jlichmanova/master-thesis-pruning-dnn/mip-env/lib/python3.10/site-packages (from nbconvert->jupyter) (3.0.2)\n",
      "Requirement already satisfied: nbclient>=0.5.0 in /home/jlichmanova/master-thesis-pruning-dnn/mip-env/lib/python3.10/site-packages (from nbconvert->jupyter) (0.10.1)\n",
      "Requirement already satisfied: bleach!=5.0.0 in /home/jlichmanova/master-thesis-pruning-dnn/mip-env/lib/python3.10/site-packages (from nbconvert->jupyter) (6.2.0)\n",
      "Requirement already satisfied: jupyterlab-pygments in /home/jlichmanova/master-thesis-pruning-dnn/mip-env/lib/python3.10/site-packages (from nbconvert->jupyter) (0.3.0)\n",
      "Requirement already satisfied: pandocfilters>=1.4.1 in /home/jlichmanova/master-thesis-pruning-dnn/mip-env/lib/python3.10/site-packages (from nbconvert->jupyter) (1.5.1)\n",
      "Requirement already satisfied: nbformat>=5.7 in /home/jlichmanova/master-thesis-pruning-dnn/mip-env/lib/python3.10/site-packages (from nbconvert->jupyter) (5.10.4)\n",
      "Requirement already satisfied: tinycss2 in /home/jlichmanova/master-thesis-pruning-dnn/mip-env/lib/python3.10/site-packages (from nbconvert->jupyter) (1.4.0)\n",
      "Requirement already satisfied: beautifulsoup4 in /home/jlichmanova/master-thesis-pruning-dnn/mip-env/lib/python3.10/site-packages (from nbconvert->jupyter) (4.12.3)\n",
      "Requirement already satisfied: defusedxml in /home/jlichmanova/master-thesis-pruning-dnn/mip-env/lib/python3.10/site-packages (from nbconvert->jupyter) (0.7.1)\n",
      "Requirement already satisfied: webencodings in /home/jlichmanova/master-thesis-pruning-dnn/mip-env/lib/python3.10/site-packages (from bleach!=5.0.0->nbconvert->jupyter) (0.5.1)\n",
      "Requirement already satisfied: certifi in /home/jlichmanova/master-thesis-pruning-dnn/mip-env/lib/python3.10/site-packages (from httpx>=0.25.0->jupyterlab->jupyter) (2024.8.30)\n",
      "Requirement already satisfied: httpcore==1.* in /home/jlichmanova/master-thesis-pruning-dnn/mip-env/lib/python3.10/site-packages (from httpx>=0.25.0->jupyterlab->jupyter) (1.0.7)\n",
      "Requirement already satisfied: idna in /home/jlichmanova/master-thesis-pruning-dnn/mip-env/lib/python3.10/site-packages (from httpx>=0.25.0->jupyterlab->jupyter) (3.10)\n",
      "Requirement already satisfied: anyio in /home/jlichmanova/master-thesis-pruning-dnn/mip-env/lib/python3.10/site-packages (from httpx>=0.25.0->jupyterlab->jupyter) (4.6.2.post1)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /home/jlichmanova/master-thesis-pruning-dnn/mip-env/lib/python3.10/site-packages (from httpcore==1.*->httpx>=0.25.0->jupyterlab->jupyter) (0.14.0)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.4 in /home/jlichmanova/master-thesis-pruning-dnn/mip-env/lib/python3.10/site-packages (from jedi>=0.16->ipython>=6.1.0->ipywidgets) (0.8.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /home/jlichmanova/master-thesis-pruning-dnn/mip-env/lib/python3.10/site-packages (from jupyter-client>=6.1.12->ipykernel->jupyter) (2.9.0.post0)\n",
      "Requirement already satisfied: platformdirs>=2.5 in /home/jlichmanova/master-thesis-pruning-dnn/mip-env/lib/python3.10/site-packages (from jupyter-core!=5.0.*,>=4.12->ipykernel->jupyter) (4.3.6)\n",
      "Requirement already satisfied: websocket-client>=1.7 in /home/jlichmanova/master-thesis-pruning-dnn/mip-env/lib/python3.10/site-packages (from jupyter-server<3,>=2.4.0->jupyterlab->jupyter) (1.8.0)\n",
      "Requirement already satisfied: argon2-cffi>=21.1 in /home/jlichmanova/master-thesis-pruning-dnn/mip-env/lib/python3.10/site-packages (from jupyter-server<3,>=2.4.0->jupyterlab->jupyter) (23.1.0)\n",
      "Requirement already satisfied: overrides>=5.0 in /home/jlichmanova/master-thesis-pruning-dnn/mip-env/lib/python3.10/site-packages (from jupyter-server<3,>=2.4.0->jupyterlab->jupyter) (7.7.0)\n",
      "Requirement already satisfied: jupyter-server-terminals>=0.4.4 in /home/jlichmanova/master-thesis-pruning-dnn/mip-env/lib/python3.10/site-packages (from jupyter-server<3,>=2.4.0->jupyterlab->jupyter) (0.5.3)\n",
      "Requirement already satisfied: prometheus-client>=0.9 in /home/jlichmanova/master-thesis-pruning-dnn/mip-env/lib/python3.10/site-packages (from jupyter-server<3,>=2.4.0->jupyterlab->jupyter) (0.21.0)\n",
      "Requirement already satisfied: terminado>=0.8.3 in /home/jlichmanova/master-thesis-pruning-dnn/mip-env/lib/python3.10/site-packages (from jupyter-server<3,>=2.4.0->jupyterlab->jupyter) (0.18.1)\n",
      "Requirement already satisfied: jupyter-events>=0.9.0 in /home/jlichmanova/master-thesis-pruning-dnn/mip-env/lib/python3.10/site-packages (from jupyter-server<3,>=2.4.0->jupyterlab->jupyter) (0.10.0)\n",
      "Requirement already satisfied: send2trash>=1.8.2 in /home/jlichmanova/master-thesis-pruning-dnn/mip-env/lib/python3.10/site-packages (from jupyter-server<3,>=2.4.0->jupyterlab->jupyter) (1.8.3)\n",
      "Requirement already satisfied: json5>=0.9.0 in /home/jlichmanova/master-thesis-pruning-dnn/mip-env/lib/python3.10/site-packages (from jupyterlab-server<3,>=2.27.1->jupyterlab->jupyter) (0.10.0)\n",
      "Requirement already satisfied: babel>=2.10 in /home/jlichmanova/master-thesis-pruning-dnn/mip-env/lib/python3.10/site-packages (from jupyterlab-server<3,>=2.27.1->jupyterlab->jupyter) (2.16.0)\n",
      "Requirement already satisfied: jsonschema>=4.18.0 in /home/jlichmanova/master-thesis-pruning-dnn/mip-env/lib/python3.10/site-packages (from jupyterlab-server<3,>=2.27.1->jupyterlab->jupyter) (4.23.0)\n",
      "Requirement already satisfied: requests>=2.31 in /home/jlichmanova/master-thesis-pruning-dnn/mip-env/lib/python3.10/site-packages (from jupyterlab-server<3,>=2.27.1->jupyterlab->jupyter) (2.32.3)\n",
      "Requirement already satisfied: fastjsonschema>=2.15 in /home/jlichmanova/master-thesis-pruning-dnn/mip-env/lib/python3.10/site-packages (from nbformat>=5.7->nbconvert->jupyter) (2.21.0)\n",
      "Requirement already satisfied: ptyprocess>=0.5 in /home/jlichmanova/master-thesis-pruning-dnn/mip-env/lib/python3.10/site-packages (from pexpect>4.3->ipython>=6.1.0->ipywidgets) (0.7.0)\n",
      "Requirement already satisfied: wcwidth in /home/jlichmanova/master-thesis-pruning-dnn/mip-env/lib/python3.10/site-packages (from prompt_toolkit<3.1.0,>=3.0.41->ipython>=6.1.0->ipywidgets) (0.2.13)\n",
      "Requirement already satisfied: soupsieve>1.2 in /home/jlichmanova/master-thesis-pruning-dnn/mip-env/lib/python3.10/site-packages (from beautifulsoup4->nbconvert->jupyter) (2.6)\n",
      "Requirement already satisfied: executing>=1.2.0 in /home/jlichmanova/master-thesis-pruning-dnn/mip-env/lib/python3.10/site-packages (from stack_data->ipython>=6.1.0->ipywidgets) (2.1.0)\n",
      "Requirement already satisfied: pure-eval in /home/jlichmanova/master-thesis-pruning-dnn/mip-env/lib/python3.10/site-packages (from stack_data->ipython>=6.1.0->ipywidgets) (0.2.3)\n",
      "Requirement already satisfied: asttokens>=2.1.0 in /home/jlichmanova/master-thesis-pruning-dnn/mip-env/lib/python3.10/site-packages (from stack_data->ipython>=6.1.0->ipywidgets) (3.0.0)\n",
      "Requirement already satisfied: sniffio>=1.1 in /home/jlichmanova/master-thesis-pruning-dnn/mip-env/lib/python3.10/site-packages (from anyio->httpx>=0.25.0->jupyterlab->jupyter) (1.3.1)\n",
      "Requirement already satisfied: argon2-cffi-bindings in /home/jlichmanova/master-thesis-pruning-dnn/mip-env/lib/python3.10/site-packages (from argon2-cffi>=21.1->jupyter-server<3,>=2.4.0->jupyterlab->jupyter) (21.2.0)\n",
      "Requirement already satisfied: attrs>=22.2.0 in /home/jlichmanova/master-thesis-pruning-dnn/mip-env/lib/python3.10/site-packages (from jsonschema>=4.18.0->jupyterlab-server<3,>=2.27.1->jupyterlab->jupyter) (24.2.0)\n",
      "Requirement already satisfied: referencing>=0.28.4 in /home/jlichmanova/master-thesis-pruning-dnn/mip-env/lib/python3.10/site-packages (from jsonschema>=4.18.0->jupyterlab-server<3,>=2.27.1->jupyterlab->jupyter) (0.35.1)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /home/jlichmanova/master-thesis-pruning-dnn/mip-env/lib/python3.10/site-packages (from jsonschema>=4.18.0->jupyterlab-server<3,>=2.27.1->jupyterlab->jupyter) (2024.10.1)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in /home/jlichmanova/master-thesis-pruning-dnn/mip-env/lib/python3.10/site-packages (from jsonschema>=4.18.0->jupyterlab-server<3,>=2.27.1->jupyterlab->jupyter) (0.21.0)\n",
      "Requirement already satisfied: python-json-logger>=2.0.4 in /home/jlichmanova/master-thesis-pruning-dnn/mip-env/lib/python3.10/site-packages (from jupyter-events>=0.9.0->jupyter-server<3,>=2.4.0->jupyterlab->jupyter) (2.0.7)\n",
      "Requirement already satisfied: rfc3986-validator>=0.1.1 in /home/jlichmanova/master-thesis-pruning-dnn/mip-env/lib/python3.10/site-packages (from jupyter-events>=0.9.0->jupyter-server<3,>=2.4.0->jupyterlab->jupyter) (0.1.1)\n",
      "Requirement already satisfied: rfc3339-validator in /home/jlichmanova/master-thesis-pruning-dnn/mip-env/lib/python3.10/site-packages (from jupyter-events>=0.9.0->jupyter-server<3,>=2.4.0->jupyterlab->jupyter) (0.1.4)\n",
      "Requirement already satisfied: pyyaml>=5.3 in /home/jlichmanova/master-thesis-pruning-dnn/mip-env/lib/python3.10/site-packages (from jupyter-events>=0.9.0->jupyter-server<3,>=2.4.0->jupyterlab->jupyter) (6.0.2)\n",
      "Requirement already satisfied: six>=1.5 in /home/jlichmanova/master-thesis-pruning-dnn/mip-env/lib/python3.10/site-packages (from python-dateutil>=2.8.2->jupyter-client>=6.1.12->ipykernel->jupyter) (1.16.0)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/jlichmanova/master-thesis-pruning-dnn/mip-env/lib/python3.10/site-packages (from requests>=2.31->jupyterlab-server<3,>=2.27.1->jupyterlab->jupyter) (2.2.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/jlichmanova/master-thesis-pruning-dnn/mip-env/lib/python3.10/site-packages (from requests>=2.31->jupyterlab-server<3,>=2.27.1->jupyterlab->jupyter) (3.4.0)\n",
      "Requirement already satisfied: jsonpointer>1.13 in /home/jlichmanova/master-thesis-pruning-dnn/mip-env/lib/python3.10/site-packages (from jsonschema>=4.18.0->jupyterlab-server<3,>=2.27.1->jupyterlab->jupyter) (3.0.0)\n",
      "Requirement already satisfied: fqdn in /home/jlichmanova/master-thesis-pruning-dnn/mip-env/lib/python3.10/site-packages (from jsonschema>=4.18.0->jupyterlab-server<3,>=2.27.1->jupyterlab->jupyter) (1.5.1)\n",
      "Requirement already satisfied: isoduration in /home/jlichmanova/master-thesis-pruning-dnn/mip-env/lib/python3.10/site-packages (from jsonschema>=4.18.0->jupyterlab-server<3,>=2.27.1->jupyterlab->jupyter) (20.11.0)\n",
      "Requirement already satisfied: webcolors>=24.6.0 in /home/jlichmanova/master-thesis-pruning-dnn/mip-env/lib/python3.10/site-packages (from jsonschema>=4.18.0->jupyterlab-server<3,>=2.27.1->jupyterlab->jupyter) (24.11.1)\n",
      "Requirement already satisfied: uri-template in /home/jlichmanova/master-thesis-pruning-dnn/mip-env/lib/python3.10/site-packages (from jsonschema>=4.18.0->jupyterlab-server<3,>=2.27.1->jupyterlab->jupyter) (1.3.0)\n",
      "Requirement already satisfied: cffi>=1.0.1 in /home/jlichmanova/master-thesis-pruning-dnn/mip-env/lib/python3.10/site-packages (from argon2-cffi-bindings->argon2-cffi>=21.1->jupyter-server<3,>=2.4.0->jupyterlab->jupyter) (1.15.1)\n",
      "Requirement already satisfied: pycparser in /home/jlichmanova/master-thesis-pruning-dnn/mip-env/lib/python3.10/site-packages (from cffi>=1.0.1->argon2-cffi-bindings->argon2-cffi>=21.1->jupyter-server<3,>=2.4.0->jupyterlab->jupyter) (2.22)\n",
      "Requirement already satisfied: arrow>=0.15.0 in /home/jlichmanova/master-thesis-pruning-dnn/mip-env/lib/python3.10/site-packages (from isoduration->jsonschema>=4.18.0->jupyterlab-server<3,>=2.27.1->jupyterlab->jupyter) (1.3.0)\n",
      "Requirement already satisfied: types-python-dateutil>=2.8.10 in /home/jlichmanova/master-thesis-pruning-dnn/mip-env/lib/python3.10/site-packages (from arrow>=0.15.0->isoduration->jsonschema>=4.18.0->jupyterlab-server<3,>=2.27.1->jupyterlab->jupyter) (2.9.0.20241003)\n",
      "usage: jupyter [-h] [--version] [--config-dir] [--data-dir] [--runtime-dir]\n",
      "               [--paths] [--json] [--debug]\n",
      "               [subcommand]\n",
      "\n",
      "Jupyter: Interactive Computing\n",
      "\n",
      "positional arguments:\n",
      "  subcommand     the subcommand to launch\n",
      "\n",
      "options:\n",
      "  -h, --help     show this help message and exit\n",
      "  --version      show the versions of core jupyter packages and exit\n",
      "  --config-dir   show Jupyter config dir\n",
      "  --data-dir     show Jupyter data dir\n",
      "  --runtime-dir  show Jupyter runtime dir\n",
      "  --paths        show all Jupyter paths. Add --json for machine-readable\n",
      "                 format.\n",
      "  --json         output paths as machine-readable json\n",
      "  --debug        output debug information about paths\n",
      "\n",
      "Available subcommands: console dejavu events execute kernel kernelspec lab\n",
      "labextension labhub migrate nbconvert notebook run server troubleshoot trust\n",
      "\n",
      "Jupyter command `jupyter-nbextension` not found.\n",
      "\u001b[33m(Deprecated) Installing extensions with the jupyter labextension install command is now deprecated and will be removed in a future major version of JupyterLab.\n",
      "\n",
      "Users should manage prebuilt extensions with package managers like pip and conda, and extension authors are encouraged to distribute their extensions as prebuilt packages \u001b[0m\n",
      "/home/jlichmanova/master-thesis-pruning-dnn/mip-env/lib/python3.10/site-packages/jupyterlab/debuglog.py:54: UserWarning: An error occurred.\n",
      "  warnings.warn(\"An error occurred.\")\n",
      "/home/jlichmanova/master-thesis-pruning-dnn/mip-env/lib/python3.10/site-packages/jupyterlab/debuglog.py:55: UserWarning: ValueError: Please install Node.js and npm before continuing installation. You may be able to install Node.js from your package manager, from conda, or directly from the Node.js website (https://nodejs.org).\n",
      "  warnings.warn(msg[-1].strip())\n",
      "/home/jlichmanova/master-thesis-pruning-dnn/mip-env/lib/python3.10/site-packages/jupyterlab/debuglog.py:56: UserWarning: See the log file for details: /tmp/jupyterlab-debug-l_j5u6s6.log\n",
      "  warnings.warn(f\"See the log file for details: {log_path!s}\")\n",
      "usage: jupyter [-h] [--version] [--config-dir] [--data-dir] [--runtime-dir]\n",
      "               [--paths] [--json] [--debug]\n",
      "               [subcommand]\n",
      "\n",
      "Jupyter: Interactive Computing\n",
      "\n",
      "positional arguments:\n",
      "  subcommand     the subcommand to launch\n",
      "\n",
      "options:\n",
      "  -h, --help     show this help message and exit\n",
      "  --version      show the versions of core jupyter packages and exit\n",
      "  --config-dir   show Jupyter config dir\n",
      "  --data-dir     show Jupyter data dir\n",
      "  --runtime-dir  show Jupyter runtime dir\n",
      "  --paths        show all Jupyter paths. Add --json for machine-readable\n",
      "                 format.\n",
      "  --json         output paths as machine-readable json\n",
      "  --debug        output debug information about paths\n",
      "\n",
      "Available subcommands: console dejavu events execute kernel kernelspec lab\n",
      "labextension labhub migrate nbconvert notebook run server troubleshoot trust\n",
      "\n",
      "Jupyter command `jupyter-nbextension` not found.\n",
      "4.2.6\n",
      "Selected Jupyter core packages...\n",
      "IPython          : 8.30.0\n",
      "ipykernel        : 6.29.5\n",
      "ipywidgets       : 8.1.5\n",
      "jupyter_client   : 8.6.3\n",
      "jupyter_core     : 5.7.2\n",
      "jupyter_server   : 2.14.2\n",
      "jupyterlab       : 4.2.6\n",
      "nbclient         : 0.10.1\n",
      "nbconvert        : 7.16.4\n",
      "nbformat         : 5.10.4\n",
      "notebook         : 7.2.2\n",
      "qtconsole        : not installed\n",
      "traitlets        : 5.14.3\n",
      "Requirement already satisfied: scikit-learn in /home/jlichmanova/master-thesis-pruning-dnn/mip-env/lib/python3.10/site-packages (1.6.1)\n",
      "Requirement already satisfied: scipy>=1.6.0 in /home/jlichmanova/master-thesis-pruning-dnn/mip-env/lib/python3.10/site-packages (from scikit-learn) (1.15.1)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /home/jlichmanova/master-thesis-pruning-dnn/mip-env/lib/python3.10/site-packages (from scikit-learn) (3.5.0)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /home/jlichmanova/master-thesis-pruning-dnn/mip-env/lib/python3.10/site-packages (from scikit-learn) (1.4.2)\n",
      "Requirement already satisfied: numpy>=1.19.5 in /home/jlichmanova/master-thesis-pruning-dnn/mip-env/lib/python3.10/site-packages (from scikit-learn) (2.1.3)\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade jupyter ipywidgets\n",
    "!jupyter nbextension enable --py widgetsnbextension\n",
    "!jupyter labextension install @jupyter-widgets/jupyterlab-manager\n",
    "!jupyter nbextension install --py widgetsnbextension\n",
    "!jupyter lab --version\n",
    "!jupyter --version\n",
    "!pip install scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "65f25b02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['runs', 'output', 'cosine_dist_mip.py', 'abs_output', '__pycache__']\n"
     ]
    }
   ],
   "source": [
    "IMAGENET_PATH = \"/data/imagenet/imagenet/\"\n",
    "\n",
    "sys.path.append(os.path.abspath('../clustering_algorithms/correlation_algorithms/whole_correlation'))\n",
    "print(os.listdir('/home/jlichmanova/master-thesis-pruning-dnn/clustering_algorithms/correlation_algorithms/whole_correlation'))\n",
    "\n",
    "from cosine_dist_mip import compute_clustering_permutation_matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "542d38e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_seed(seed=42, rank=0):\n",
    "    torch.manual_seed(seed + rank)\n",
    "    np.random.seed(seed + rank)\n",
    "    random.seed(seed + rank)\n",
    "\n",
    "random_seed(47)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6c6788d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\")\n",
    "amp_autocast = partial(torch.autocast, device_type=device.type, dtype=torch.float16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bb161d19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VisionTransformer(\n",
      "  (patch_embed): PatchEmbed(\n",
      "    (proj): Conv2d(3, 96, kernel_size=(16, 16), stride=(16, 16))\n",
      "    (norm): Identity()\n",
      "  )\n",
      "  (pos_drop): Dropout(p=0.0, inplace=False)\n",
      "  (patch_drop): Identity()\n",
      "  (norm_pre): Identity()\n",
      "  (blocks): Sequential(\n",
      "    (0): Block(\n",
      "      (norm1): LayerNorm((96,), eps=1e-06, elementwise_affine=True)\n",
      "      (attn): Attention(\n",
      "        (qkv): Linear(in_features=96, out_features=288, bias=True)\n",
      "        (q_norm): Identity()\n",
      "        (k_norm): Identity()\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(in_features=96, out_features=96, bias=True)\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ls1): LayerScale()\n",
      "      (drop_path1): Identity()\n",
      "      (norm2): LayerNorm((96,), eps=1e-06, elementwise_affine=True)\n",
      "      (mlp): Mlp(\n",
      "        (fc1): Linear(in_features=96, out_features=192, bias=True)\n",
      "        (act): GELU(approximate='none')\n",
      "        (drop1): Dropout(p=0.0, inplace=False)\n",
      "        (norm): Identity()\n",
      "        (fc2): Linear(in_features=192, out_features=96, bias=True)\n",
      "        (drop2): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ls2): LayerScale()\n",
      "      (drop_path2): Identity()\n",
      "    )\n",
      "    (1): Block(\n",
      "      (norm1): LayerNorm((96,), eps=1e-06, elementwise_affine=True)\n",
      "      (attn): Attention(\n",
      "        (qkv): Linear(in_features=96, out_features=288, bias=True)\n",
      "        (q_norm): Identity()\n",
      "        (k_norm): Identity()\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(in_features=96, out_features=96, bias=True)\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ls1): LayerScale()\n",
      "      (drop_path1): Identity()\n",
      "      (norm2): LayerNorm((96,), eps=1e-06, elementwise_affine=True)\n",
      "      (mlp): Mlp(\n",
      "        (fc1): Linear(in_features=96, out_features=192, bias=True)\n",
      "        (act): GELU(approximate='none')\n",
      "        (drop1): Dropout(p=0.0, inplace=False)\n",
      "        (norm): Identity()\n",
      "        (fc2): Linear(in_features=192, out_features=96, bias=True)\n",
      "        (drop2): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ls2): LayerScale()\n",
      "      (drop_path2): Identity()\n",
      "    )\n",
      "    (2): Block(\n",
      "      (norm1): LayerNorm((96,), eps=1e-06, elementwise_affine=True)\n",
      "      (attn): Attention(\n",
      "        (qkv): Linear(in_features=96, out_features=288, bias=True)\n",
      "        (q_norm): Identity()\n",
      "        (k_norm): Identity()\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(in_features=96, out_features=96, bias=True)\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ls1): LayerScale()\n",
      "      (drop_path1): Identity()\n",
      "      (norm2): LayerNorm((96,), eps=1e-06, elementwise_affine=True)\n",
      "      (mlp): Mlp(\n",
      "        (fc1): Linear(in_features=96, out_features=192, bias=True)\n",
      "        (act): GELU(approximate='none')\n",
      "        (drop1): Dropout(p=0.0, inplace=False)\n",
      "        (norm): Identity()\n",
      "        (fc2): Linear(in_features=192, out_features=96, bias=True)\n",
      "        (drop2): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ls2): LayerScale()\n",
      "      (drop_path2): Identity()\n",
      "    )\n",
      "    (3): Block(\n",
      "      (norm1): LayerNorm((96,), eps=1e-06, elementwise_affine=True)\n",
      "      (attn): Attention(\n",
      "        (qkv): Linear(in_features=96, out_features=288, bias=True)\n",
      "        (q_norm): Identity()\n",
      "        (k_norm): Identity()\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(in_features=96, out_features=96, bias=True)\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ls1): LayerScale()\n",
      "      (drop_path1): Identity()\n",
      "      (norm2): LayerNorm((96,), eps=1e-06, elementwise_affine=True)\n",
      "      (mlp): Mlp(\n",
      "        (fc1): Linear(in_features=96, out_features=192, bias=True)\n",
      "        (act): GELU(approximate='none')\n",
      "        (drop1): Dropout(p=0.0, inplace=False)\n",
      "        (norm): Identity()\n",
      "        (fc2): Linear(in_features=192, out_features=96, bias=True)\n",
      "        (drop2): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ls2): LayerScale()\n",
      "      (drop_path2): Identity()\n",
      "    )\n",
      "    (4): Block(\n",
      "      (norm1): LayerNorm((96,), eps=1e-06, elementwise_affine=True)\n",
      "      (attn): Attention(\n",
      "        (qkv): Linear(in_features=96, out_features=288, bias=True)\n",
      "        (q_norm): Identity()\n",
      "        (k_norm): Identity()\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(in_features=96, out_features=96, bias=True)\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ls1): LayerScale()\n",
      "      (drop_path1): Identity()\n",
      "      (norm2): LayerNorm((96,), eps=1e-06, elementwise_affine=True)\n",
      "      (mlp): Mlp(\n",
      "        (fc1): Linear(in_features=96, out_features=192, bias=True)\n",
      "        (act): GELU(approximate='none')\n",
      "        (drop1): Dropout(p=0.0, inplace=False)\n",
      "        (norm): Identity()\n",
      "        (fc2): Linear(in_features=192, out_features=96, bias=True)\n",
      "        (drop2): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ls2): LayerScale()\n",
      "      (drop_path2): Identity()\n",
      "    )\n",
      "    (5): Block(\n",
      "      (norm1): LayerNorm((96,), eps=1e-06, elementwise_affine=True)\n",
      "      (attn): Attention(\n",
      "        (qkv): Linear(in_features=96, out_features=288, bias=True)\n",
      "        (q_norm): Identity()\n",
      "        (k_norm): Identity()\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(in_features=96, out_features=96, bias=True)\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ls1): LayerScale()\n",
      "      (drop_path1): Identity()\n",
      "      (norm2): LayerNorm((96,), eps=1e-06, elementwise_affine=True)\n",
      "      (mlp): Mlp(\n",
      "        (fc1): Linear(in_features=96, out_features=192, bias=True)\n",
      "        (act): GELU(approximate='none')\n",
      "        (drop1): Dropout(p=0.0, inplace=False)\n",
      "        (norm): Identity()\n",
      "        (fc2): Linear(in_features=192, out_features=96, bias=True)\n",
      "        (drop2): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ls2): LayerScale()\n",
      "      (drop_path2): Identity()\n",
      "    )\n",
      "    (6): Block(\n",
      "      (norm1): LayerNorm((96,), eps=1e-06, elementwise_affine=True)\n",
      "      (attn): Attention(\n",
      "        (qkv): Linear(in_features=96, out_features=288, bias=True)\n",
      "        (q_norm): Identity()\n",
      "        (k_norm): Identity()\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(in_features=96, out_features=96, bias=True)\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ls1): LayerScale()\n",
      "      (drop_path1): Identity()\n",
      "      (norm2): LayerNorm((96,), eps=1e-06, elementwise_affine=True)\n",
      "      (mlp): Mlp(\n",
      "        (fc1): Linear(in_features=96, out_features=192, bias=True)\n",
      "        (act): GELU(approximate='none')\n",
      "        (drop1): Dropout(p=0.0, inplace=False)\n",
      "        (norm): Identity()\n",
      "        (fc2): Linear(in_features=192, out_features=96, bias=True)\n",
      "        (drop2): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ls2): LayerScale()\n",
      "      (drop_path2): Identity()\n",
      "    )\n",
      "    (7): Block(\n",
      "      (norm1): LayerNorm((96,), eps=1e-06, elementwise_affine=True)\n",
      "      (attn): Attention(\n",
      "        (qkv): Linear(in_features=96, out_features=288, bias=True)\n",
      "        (q_norm): Identity()\n",
      "        (k_norm): Identity()\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(in_features=96, out_features=96, bias=True)\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ls1): LayerScale()\n",
      "      (drop_path1): Identity()\n",
      "      (norm2): LayerNorm((96,), eps=1e-06, elementwise_affine=True)\n",
      "      (mlp): Mlp(\n",
      "        (fc1): Linear(in_features=96, out_features=192, bias=True)\n",
      "        (act): GELU(approximate='none')\n",
      "        (drop1): Dropout(p=0.0, inplace=False)\n",
      "        (norm): Identity()\n",
      "        (fc2): Linear(in_features=192, out_features=96, bias=True)\n",
      "        (drop2): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ls2): LayerScale()\n",
      "      (drop_path2): Identity()\n",
      "    )\n",
      "    (8): Block(\n",
      "      (norm1): LayerNorm((96,), eps=1e-06, elementwise_affine=True)\n",
      "      (attn): Attention(\n",
      "        (qkv): Linear(in_features=96, out_features=288, bias=True)\n",
      "        (q_norm): Identity()\n",
      "        (k_norm): Identity()\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(in_features=96, out_features=96, bias=True)\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ls1): LayerScale()\n",
      "      (drop_path1): Identity()\n",
      "      (norm2): LayerNorm((96,), eps=1e-06, elementwise_affine=True)\n",
      "      (mlp): Mlp(\n",
      "        (fc1): Linear(in_features=96, out_features=192, bias=True)\n",
      "        (act): GELU(approximate='none')\n",
      "        (drop1): Dropout(p=0.0, inplace=False)\n",
      "        (norm): Identity()\n",
      "        (fc2): Linear(in_features=192, out_features=96, bias=True)\n",
      "        (drop2): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ls2): LayerScale()\n",
      "      (drop_path2): Identity()\n",
      "    )\n",
      "  )\n",
      "  (norm): LayerNorm((96,), eps=1e-06, elementwise_affine=True)\n",
      "  (attn_pool): AttentionPoolLatent(\n",
      "    (q): Linear(in_features=96, out_features=96, bias=True)\n",
      "    (kv): Linear(in_features=96, out_features=192, bias=True)\n",
      "    (q_norm): Identity()\n",
      "    (k_norm): Identity()\n",
      "    (proj): Linear(in_features=96, out_features=96, bias=True)\n",
      "    (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "    (norm): LayerNorm((96,), eps=1e-06, elementwise_affine=True)\n",
      "    (mlp): Mlp(\n",
      "      (fc1): Linear(in_features=96, out_features=192, bias=True)\n",
      "      (act): GELU(approximate='none')\n",
      "      (drop1): Dropout(p=0.0, inplace=False)\n",
      "      (norm): Identity()\n",
      "      (fc2): Linear(in_features=192, out_features=96, bias=True)\n",
      "      (drop2): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "  )\n",
      "  (fc_norm): Identity()\n",
      "  (head_drop): Dropout(p=0.0, inplace=False)\n",
      "  (head): Linear(in_features=96, out_features=1000, bias=True)\n",
      ")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "930280"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Very small models\n",
    "model_name = \"test_vit3.r160_in1k\"\n",
    "#model_name = \"test_vit2.r160_in1k\"\n",
    "#model_name = \"test_vit.r160_in1k\"\n",
    "\n",
    "# Bigger models\n",
    "#model_name = \"vit_wee_patch16_reg1_gap_256.sbb_in1k\" \n",
    "#model_name = \"vit_medium_patch16_reg4_gap_256.sbb_in1k\"\n",
    "\n",
    "# Even bigger\n",
    "# See bigger than tiny from: https://huggingface.co/timm/convnext_tiny.fb_in1k\n",
    "# Or others at https://huggingface.co/timm/vit_wee_patch16_reg1_gap_256.sbb_in1k\n",
    "\n",
    "model = timm.create_model(model_name, pretrained=True)\n",
    "model.cuda() \n",
    "print(model)\n",
    "\n",
    "sum(p.numel() for p in model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "470df42a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(model, val_loader):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for input, target in tqdm(val_loader):\n",
    "            with amp_autocast():\n",
    "                output = model(input)\n",
    "                correct += (target == output.argmax(dim=1)).sum().item()\n",
    "                total += target.numel()\n",
    "    print(\"val acc\", correct / total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fb277102",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_size': (3, 160, 160), 'interpolation': 'bicubic', 'mean': (0.5, 0.5, 0.5), 'std': (0.5, 0.5, 0.5), 'crop_pct': 0.95, 'crop_mode': 'center'}\n",
      "torch.Size([128, 3, 160, 160]) torch.Size([128])\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
      "        2, 2, 2, 2, 2, 2, 2, 2], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "data_config = timm.data.resolve_data_config(model=model)\n",
    "\n",
    "print(data_config)\n",
    "\n",
    "val_dataset = timm.data.create_dataset(\n",
    "    name=\"imagenet\",\n",
    "    split=\"validation\",\n",
    "    root=IMAGENET_PATH\n",
    ")\n",
    "\n",
    "val_loader = timm.data.create_loader(\n",
    "    val_dataset,\n",
    "    input_size=data_config['input_size'],                                                 \n",
    "    batch_size=128,\n",
    "    use_prefetcher=True,                                                       \n",
    "    interpolation=data_config['interpolation'],                                           \n",
    "    mean=data_config['mean'],\n",
    "    std=data_config['std'],\n",
    "    num_workers=8,                                                             \n",
    "    crop_pct=data_config[\"crop_pct\"],\n",
    "    crop_mode=data_config['crop_mode'],                                                   \n",
    "    crop_border_pixels=False,  \n",
    "    pin_memory=True,\n",
    "    device=device,\n",
    ")\n",
    "\n",
    "for bx, by in val_loader:\n",
    "    print(bx.shape, by.shape)\n",
    "    print(by)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "868de487",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jlichmanova/master-thesis-pruning-dnn/mip-env/lib/python3.10/site-packages/torch/utils/data/dataloader.py:617: UserWarning: This DataLoader will create 12 worker processes in total. Our suggested max number of worker in current system is 8, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "train_dataset = timm.data.create_dataset(\n",
    "    name=\"imagenet\",\n",
    "    split=\"train\",\n",
    "    root=IMAGENET_PATH\n",
    ")\n",
    "\n",
    "# TODO: augmentations\n",
    "train_loader = timm.data.create_loader(\n",
    "    train_dataset,\n",
    "    input_size=data_config['input_size'],                                                 \n",
    "    batch_size=128,\n",
    "    use_prefetcher=True,                                                       \n",
    "    interpolation=\"random\",  \n",
    "    mean=data_config['mean'],\n",
    "    std=data_config['std'],\n",
    "    num_workers=12,                                                             \n",
    "    crop_pct=data_config[\"crop_pct\"],\n",
    "    crop_mode=data_config['crop_mode'],                                                   \n",
    "    crop_border_pixels=False,  \n",
    "    pin_memory=True,\n",
    "    device=device,\n",
    "    is_training=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f579152f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "59d36d0f8321406997ddc3176f69e7cf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/391 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val acc 0.5692\n"
     ]
    }
   ],
   "source": [
    "# Run validation on full model (reports number in range 0-1)\n",
    "\n",
    "validate(model, val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "c8be7458",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for gathering input statistics for a layer\n",
    "def update_cov(m, i, o):\n",
    "    x = i[0].detach().flatten(0, -2).float()\n",
    "    with torch.autocast(device_type=\"cuda\", enabled=False):\n",
    "        m.XX.data += x.square().sum(dim=0)\n",
    "\n",
    "# Selection functions for picking which layers to compress\n",
    "# In general, we do not compress first and last layer\n",
    "# Name of the layer is n.n2 (where there are no dots in n2)\n",
    "# Actual layer is m\n",
    "select_all_linear = lambda n, n2, m: type(m) == nn.Linear and \"head\" not in n and \"head\" not in n2\n",
    "select_only_attn_proj = lambda n, n2, m: type(m) == nn.Linear and \"head\" not in n and \"head\" not in n2 and \"proj\" in n2\n",
    "select = lambda n, n2, m: type(m) == nn.Linear and \"blocks.1\" in n and \"proj\" in n2\n",
    "\n",
    "def process_model(model_name=model_name, replace_filter=select, \n",
    "                  replace_fn=lambda x: x, finetune_epochs=2, finetune_maxlr=1e-5,\n",
    "                  wanda=False):\n",
    "    \"\"\"\n",
    "    :model_name: Name of model\n",
    "    :replace_filter: Which layers to replace\n",
    "    :replace_fn: Takes a layer, outputs new layer (or sequence of layers)\n",
    "    :finetune_epochs: How many epochs to use for finetuning\n",
    "    :finetune_maxlr: Maximal learning rate for finetuning\n",
    "    :wanda: Whether to collect wanda statistics (if False, norms of inputs will be one)\n",
    "    \"\"\"\n",
    "    model = timm.create_model(model_name, pretrained=True)\n",
    "    model.cuda()\n",
    "    \n",
    "    print(\"pars before\", sum(p.numel() for p in model.parameters()))\n",
    "    \n",
    "    # need to dump, because changing during iteration would make a mess\n",
    "    to_change = []\n",
    "    for n, m in model.named_modules():\n",
    "        for n2, m2 in m.named_modules():\n",
    "            if \".\" not in n2 and replace_filter(n, n2, m2) and len(n2) > 0:\n",
    "                to_change.append((m, n2, m2))\n",
    "                if wanda:\n",
    "                    m2.XX = torch.zeros((m2.weight.shape[1]), device=m2.weight.device)\n",
    "                    m2.register_forward_hook(update_cov)\n",
    "                else:\n",
    "                    m2.XX = torch.ones((m2.weight.shape[1]), device=m2.weight.device)\n",
    "                \n",
    "    \n",
    "    # Gather input stats\n",
    "    if wanda:\n",
    "        step = 0\n",
    "        for input, _ in train_loader:\n",
    "            with amp_autocast():\n",
    "                with torch.no_grad():\n",
    "                    model(input)\n",
    "            step += 1\n",
    "            if step == 50:\n",
    "                break\n",
    "    \n",
    "    for m, n2, m2 in to_change:\n",
    "        setattr(m, n2, replace_fn(m2))\n",
    "    \n",
    "    print(\"pars after\", sum(p.numel() for p in model.parameters()))\n",
    "    print(\"first validation\")\n",
    "    #validate(model, val_loader)\n",
    "    \n",
    "    if finetune_epochs == 0:\n",
    "        return\n",
    "    \n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=finetune_maxlr, weight_decay=1e-4)\n",
    "    scheduler = torch.optim.lr_scheduler.PolynomialLR(optimizer, total_iters=len(train_loader)*finetune_epochs)\n",
    "    scaler = torch.GradScaler(\"cuda\")\n",
    "\n",
    "    for ep in range(finetune_epochs):\n",
    "        model.train()\n",
    "        loss_sum = 0\n",
    "        loss_cc = 0\n",
    "        for input, target in (pbar := tqdm(train_loader)):\n",
    "            with amp_autocast():\n",
    "                output = model(input)\n",
    "                loss = F.cross_entropy(output, target)\n",
    "                scaler.scale(loss).backward()\n",
    "                scaler.step(optimizer)\n",
    "                scaler.update()\n",
    "                scheduler.step()\n",
    "                optimizer.zero_grad()\n",
    "                loss_sum += loss.item()\n",
    "                loss_cc += 1\n",
    "                pbar.set_description(\"loss: %.3f %.3f\" % (loss.item(), loss_sum / loss_cc))\n",
    "\n",
    "        validate(model, val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "1db2c103",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pars before 930280\n",
      "auto rank 6\n",
      "Linear(in_features=96, out_features=96, bias=True) tensor(1., device='cuda:0') tensor(1., device='cuda:0')\n",
      "tensor([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,\n",
      "        18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35,\n",
      "        36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53,\n",
      "        54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71,\n",
      "        72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89,\n",
      "        90, 91, 92, 93, 94, 95])\n",
      "tensor([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,\n",
      "        18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35,\n",
      "        36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53,\n",
      "        54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71,\n",
      "        72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89,\n",
      "        90, 91, 92, 93, 94, 95])\n",
      "density 0.5026041666666666\n",
      "torch.Size([96]) torch.Size([96])\n",
      "24 24\n",
      "svd 6.0900415033102036 triv 20.159584999084473\n",
      "Permute input torch.Size([96, 96])\n",
      "Permute output torch.Size([96, 96])\n",
      "Permute input torch.Size([96, 96])\n",
      "Permute output torch.Size([96, 96])\n",
      "err 0.0006608118419535458 triv err 0.002187454840168357\n",
      "Permute input torch.Size([96, 96])\n",
      "Permute output torch.Size([96, 96])\n",
      "Permute input torch.Size([96, 96])\n",
      "Permute output torch.Size([96, 96])\n",
      "err norm 0.0006608118419535458 triv err 0.002187454840168357\n",
      "pars after 925672\n",
      "first validation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3775129/518393965.py:94: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  P_rows = torch.tensor(P_rows, device=\"cuda\")\n",
      "/tmp/ipykernel_3775129/518393965.py:95: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  P_columns = torch.tensor(P_columns, device=\"cuda\")\n"
     ]
    }
   ],
   "source": [
    "class PermutationLayer(nn.Module):\n",
    "    def __init__(self, P):\n",
    "        super().__init__()\n",
    "        self.register_buffer(\"permutation_matrix\", P)\n",
    "\n",
    "    def forward(self, x):\n",
    "        print(\"Permute input\", x.shape)\n",
    "        out =  x[..., self.permutation_matrix]\n",
    "        print(\"Permute output\", out.shape)\n",
    "        return out\n",
    "\n",
    "class BlockLinear2(nn.Module):\n",
    "    def __init__(self, nb=4, bs=(128,512), bias=False, dbg=False):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.weight = nn.Parameter(torch.zeros(nb, bs[0], bs[1]))\n",
    "        self.bias = None\n",
    "        if bias:\n",
    "            self.bias = nn.Parameter(torch.zeros(bs[1]))\n",
    "    \n",
    "    def extra_repr(self):\n",
    "        return \"w_shape %s\" % str(self.weight.shape)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        orig_shape = x.shape\n",
    "        x = x.flatten(0, -2)\n",
    "        x = x.reshape(x.shape[0], self.weight.shape[0], -1).permute(1, 0, 2)\n",
    "        x = x.matmul(self.weight)\n",
    "        x = x.permute(1, 0, 2)\n",
    "        x = x.reshape(tuple(list(orig_shape[0:-1]) + [-1]))\n",
    "        if self.bias is not None:\n",
    "            x = x + self.bias\n",
    "        return x\n",
    "\n",
    "class MonarchPerm2(nn.Module):\n",
    "    def __init__(self, nb=4, size=128):\n",
    "        super().__init__()\n",
    "        out = []\n",
    "        perm_base = torch.arange(size)\n",
    "        parts = perm_base.chunk(nb*nb)\n",
    "        for i in range(nb):\n",
    "            out += parts[i::nb]\n",
    "        self.perm = torch.cat(out).cuda()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        orig_shape = x.shape\n",
    "        x = x.flatten(0, -2)\n",
    "        out = x.T[self.perm].T\n",
    "        return out.reshape(orig_shape)\n",
    "    \n",
    "class LambdaLayer(nn.Module):\n",
    "    def __init__(self, f):\n",
    "        super().__init__()\n",
    "        self.f = f\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.f(x)\n",
    "\n",
    "def replace_with_monarch(m, n_blocks=4, rank=None):\n",
    "    assert m.weight.shape[0] % n_blocks == 0\n",
    "    assert m.weight.shape[1] % n_blocks == 0\n",
    "    if rank == None:\n",
    "        # ~50% of original weights\n",
    "        bs = m.weight.shape[0] // n_blocks, m.weight.shape[1] // n_blocks\n",
    "        rank = int((bs[0] * bs[1]) / (bs[0] + bs[1]) / 2)\n",
    "        print(\"auto rank\", rank)\n",
    "    \n",
    "    norm = m.XX.sqrt() + 1e-8\n",
    "    norm = norm / norm.mean()\n",
    "    print(m, norm.amin(), norm.amax())\n",
    "    W = m.weight.detach() * norm\n",
    "    mid = rank * n_blocks\n",
    "\n",
    "    s0 = W.shape[0] // n_blocks\n",
    "    s1 = W.shape[1] // n_blocks\n",
    "    svd_err = 0\n",
    "    \n",
    "    weights_cpu = m.weight.detach().cpu()\n",
    "\n",
    "    #P_rows, P_columns, X_permuted = compute_clustering_permutation_matrices(weights_cpu, n_blocks, n_blocks, 30)\n",
    "\n",
    "    #P_rows = P_rows.nonzero()[:,1]\n",
    "    # print(P_rows)\n",
    "    #P_columns = P_columns.nonzero()[:,1]\n",
    "    # print(P_columns)\n",
    "    \n",
    "    P_rows = torch.arange(W.shape[0])\n",
    "    P_columns = torch.arange(W.shape[1])\n",
    "    #P_columns[0] = 60\n",
    "    #P_columns[60] = 0\n",
    "    print(P_columns)\n",
    "    print(P_rows)\n",
    "\n",
    "    P_rows = torch.tensor(P_rows, device=\"cuda\")\n",
    "    P_columns = torch.tensor(P_columns, device=\"cuda\")\n",
    "    \n",
    "    layer2 = nn.Sequential(\n",
    "        PermutationLayer((P_columns)),\n",
    "        BlockLinear2(n_blocks, (W.shape[1]//n_blocks, mid)), \n",
    "        MonarchPerm2(n_blocks, mid*n_blocks),\n",
    "        BlockLinear2(n_blocks, (mid, W.shape[0]//n_blocks), bias=(m.bias is not None)),\n",
    "        PermutationLayer(torch.argsort(P_rows)),\n",
    "    ).cuda()\n",
    "    \n",
    "    print(\"density\", sum(p.numel() for p in layer2.parameters()) / W.numel())\n",
    "\n",
    "    W = W[P_rows,:]\n",
    "    W = W[:,P_columns]\n",
    "    triv_err = 0\n",
    "    for i in range(n_blocks):\n",
    "        for j in range(n_blocks):\n",
    "            part = W[i*s0:i*s0+s0, j*s1:j*s1+s1]\n",
    "\n",
    "            U, s, Vh = torch.linalg.svd(part, full_matrices=False)\n",
    "            s = s[:rank]\n",
    "            U = U[:,:rank] * s.sqrt()\n",
    "            Vh = Vh[:rank] * s.sqrt().unsqueeze(1)\n",
    "            svd_err += (part - (U @ Vh)).square().sum().item()\n",
    "            triv_err += part.square().sum().item()\n",
    "            assert layer2[1].weight.data[j, :,rank*i:rank*i+rank].numel() > 0\n",
    "            assert layer2[3].weight.data[i, rank*j:rank*j+rank,:].numel() > 0\n",
    "            layer2[1].weight.data[j, :, rank*i:rank*i+rank] = (Vh / norm[j*s1:j*s1+s1]).T\n",
    "            layer2[3].weight.data[i, rank*j:rank*j+rank] = U.T\n",
    "\n",
    "    print(P_rows.shape, P_columns.shape)\n",
    "    print(s0, s1)\n",
    "    print(\"svd\", svd_err, \"triv\", triv_err)\n",
    "    \n",
    "    if m.bias is not None:\n",
    "        layer2[3].bias.data = m.bias[P_rows]\n",
    "    \n",
    "    test = torch.eye(m.weight.shape[1], device=\"cuda\")\n",
    "    if m.bias is not None:\n",
    "        print(\"err\", (layer2(test) - m(test)).square().mean().item(), \"triv err\", (m(test) - m.bias.detach()).square().mean().item())\n",
    "    else:\n",
    "        print(\"err\", (layer2(test) - m(test)).square().mean().item(), \"triv err\", (m(test)).square().mean().item())\n",
    "    test = torch.eye(m.weight.shape[1], device=\"cuda\") * norm\n",
    "    if m.bias is not None:\n",
    "        print(\"err norm\", (layer2(test) - m(test)).square().mean().item(), \"triv err\", (m(test) - m.bias.detach()).square().mean().item())\n",
    "    else:\n",
    "        print(\"err norm\", (layer2(test) - m(test)).square().mean().item(), \"triv err\", (m(test)).square().mean().item())\n",
    "    \n",
    "    return layer2\n",
    "\n",
    "process_model(replace_fn=replace_with_monarch, finetune_epochs=0, finetune_maxlr=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1becce4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "process_model(replace_fn=replace_with_monarch, finetune_epochs=0, finetune_maxlr=1e-4, wanda=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33852696",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Toto trva dlho\n",
    "# process_model(replace_fn=replace_with_monarch, finetune_epochs=1, finetune_maxlr=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "141c9079",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (mip-env)",
   "language": "python",
   "name": "mip-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
