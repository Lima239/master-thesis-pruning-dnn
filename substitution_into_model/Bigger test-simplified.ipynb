{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1608b892",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# Tu si treba vybrat jedno GPU (cize bud 0 alebo 1)\n",
    "# Je to cislovane naopak ako v nvidia-smi, because ..., that's why\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\" \n",
    "os.environ[\"OMP_NUM_THREADS\"] = \"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "65f25b02",
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGENET_PATH = \"/data/imagenet/imagenet/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8b12ab01",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import os\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets\n",
    "import time\n",
    "import copy\n",
    "import sys\n",
    "\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.init as init\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "\n",
    "import sys\n",
    "import numpy as np\n",
    "import timm\n",
    "from functools import partial\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "542d38e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_seed(seed=42, rank=0):\n",
    "    torch.manual_seed(seed + rank)\n",
    "    np.random.seed(seed + rank)\n",
    "    random.seed(seed + rank)\n",
    "\n",
    "random_seed(47)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6c6788d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\")\n",
    "amp_autocast = partial(torch.autocast, device_type=device.type, dtype=torch.float16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bb161d19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VisionTransformer(\n",
      "  (patch_embed): PatchEmbed(\n",
      "    (proj): Conv2d(3, 96, kernel_size=(16, 16), stride=(16, 16))\n",
      "    (norm): Identity()\n",
      "  )\n",
      "  (pos_drop): Dropout(p=0.0, inplace=False)\n",
      "  (patch_drop): Identity()\n",
      "  (norm_pre): Identity()\n",
      "  (blocks): Sequential(\n",
      "    (0): Block(\n",
      "      (norm1): LayerNorm((96,), eps=1e-06, elementwise_affine=True)\n",
      "      (attn): Attention(\n",
      "        (qkv): Linear(in_features=96, out_features=288, bias=True)\n",
      "        (q_norm): Identity()\n",
      "        (k_norm): Identity()\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(in_features=96, out_features=96, bias=True)\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ls1): LayerScale()\n",
      "      (drop_path1): Identity()\n",
      "      (norm2): LayerNorm((96,), eps=1e-06, elementwise_affine=True)\n",
      "      (mlp): Mlp(\n",
      "        (fc1): Linear(in_features=96, out_features=192, bias=True)\n",
      "        (act): GELU(approximate='none')\n",
      "        (drop1): Dropout(p=0.0, inplace=False)\n",
      "        (norm): Identity()\n",
      "        (fc2): Linear(in_features=192, out_features=96, bias=True)\n",
      "        (drop2): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ls2): LayerScale()\n",
      "      (drop_path2): Identity()\n",
      "    )\n",
      "    (1): Block(\n",
      "      (norm1): LayerNorm((96,), eps=1e-06, elementwise_affine=True)\n",
      "      (attn): Attention(\n",
      "        (qkv): Linear(in_features=96, out_features=288, bias=True)\n",
      "        (q_norm): Identity()\n",
      "        (k_norm): Identity()\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(in_features=96, out_features=96, bias=True)\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ls1): LayerScale()\n",
      "      (drop_path1): Identity()\n",
      "      (norm2): LayerNorm((96,), eps=1e-06, elementwise_affine=True)\n",
      "      (mlp): Mlp(\n",
      "        (fc1): Linear(in_features=96, out_features=192, bias=True)\n",
      "        (act): GELU(approximate='none')\n",
      "        (drop1): Dropout(p=0.0, inplace=False)\n",
      "        (norm): Identity()\n",
      "        (fc2): Linear(in_features=192, out_features=96, bias=True)\n",
      "        (drop2): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ls2): LayerScale()\n",
      "      (drop_path2): Identity()\n",
      "    )\n",
      "    (2): Block(\n",
      "      (norm1): LayerNorm((96,), eps=1e-06, elementwise_affine=True)\n",
      "      (attn): Attention(\n",
      "        (qkv): Linear(in_features=96, out_features=288, bias=True)\n",
      "        (q_norm): Identity()\n",
      "        (k_norm): Identity()\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(in_features=96, out_features=96, bias=True)\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ls1): LayerScale()\n",
      "      (drop_path1): Identity()\n",
      "      (norm2): LayerNorm((96,), eps=1e-06, elementwise_affine=True)\n",
      "      (mlp): Mlp(\n",
      "        (fc1): Linear(in_features=96, out_features=192, bias=True)\n",
      "        (act): GELU(approximate='none')\n",
      "        (drop1): Dropout(p=0.0, inplace=False)\n",
      "        (norm): Identity()\n",
      "        (fc2): Linear(in_features=192, out_features=96, bias=True)\n",
      "        (drop2): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ls2): LayerScale()\n",
      "      (drop_path2): Identity()\n",
      "    )\n",
      "    (3): Block(\n",
      "      (norm1): LayerNorm((96,), eps=1e-06, elementwise_affine=True)\n",
      "      (attn): Attention(\n",
      "        (qkv): Linear(in_features=96, out_features=288, bias=True)\n",
      "        (q_norm): Identity()\n",
      "        (k_norm): Identity()\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(in_features=96, out_features=96, bias=True)\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ls1): LayerScale()\n",
      "      (drop_path1): Identity()\n",
      "      (norm2): LayerNorm((96,), eps=1e-06, elementwise_affine=True)\n",
      "      (mlp): Mlp(\n",
      "        (fc1): Linear(in_features=96, out_features=192, bias=True)\n",
      "        (act): GELU(approximate='none')\n",
      "        (drop1): Dropout(p=0.0, inplace=False)\n",
      "        (norm): Identity()\n",
      "        (fc2): Linear(in_features=192, out_features=96, bias=True)\n",
      "        (drop2): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ls2): LayerScale()\n",
      "      (drop_path2): Identity()\n",
      "    )\n",
      "    (4): Block(\n",
      "      (norm1): LayerNorm((96,), eps=1e-06, elementwise_affine=True)\n",
      "      (attn): Attention(\n",
      "        (qkv): Linear(in_features=96, out_features=288, bias=True)\n",
      "        (q_norm): Identity()\n",
      "        (k_norm): Identity()\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(in_features=96, out_features=96, bias=True)\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ls1): LayerScale()\n",
      "      (drop_path1): Identity()\n",
      "      (norm2): LayerNorm((96,), eps=1e-06, elementwise_affine=True)\n",
      "      (mlp): Mlp(\n",
      "        (fc1): Linear(in_features=96, out_features=192, bias=True)\n",
      "        (act): GELU(approximate='none')\n",
      "        (drop1): Dropout(p=0.0, inplace=False)\n",
      "        (norm): Identity()\n",
      "        (fc2): Linear(in_features=192, out_features=96, bias=True)\n",
      "        (drop2): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ls2): LayerScale()\n",
      "      (drop_path2): Identity()\n",
      "    )\n",
      "    (5): Block(\n",
      "      (norm1): LayerNorm((96,), eps=1e-06, elementwise_affine=True)\n",
      "      (attn): Attention(\n",
      "        (qkv): Linear(in_features=96, out_features=288, bias=True)\n",
      "        (q_norm): Identity()\n",
      "        (k_norm): Identity()\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(in_features=96, out_features=96, bias=True)\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ls1): LayerScale()\n",
      "      (drop_path1): Identity()\n",
      "      (norm2): LayerNorm((96,), eps=1e-06, elementwise_affine=True)\n",
      "      (mlp): Mlp(\n",
      "        (fc1): Linear(in_features=96, out_features=192, bias=True)\n",
      "        (act): GELU(approximate='none')\n",
      "        (drop1): Dropout(p=0.0, inplace=False)\n",
      "        (norm): Identity()\n",
      "        (fc2): Linear(in_features=192, out_features=96, bias=True)\n",
      "        (drop2): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ls2): LayerScale()\n",
      "      (drop_path2): Identity()\n",
      "    )\n",
      "    (6): Block(\n",
      "      (norm1): LayerNorm((96,), eps=1e-06, elementwise_affine=True)\n",
      "      (attn): Attention(\n",
      "        (qkv): Linear(in_features=96, out_features=288, bias=True)\n",
      "        (q_norm): Identity()\n",
      "        (k_norm): Identity()\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(in_features=96, out_features=96, bias=True)\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ls1): LayerScale()\n",
      "      (drop_path1): Identity()\n",
      "      (norm2): LayerNorm((96,), eps=1e-06, elementwise_affine=True)\n",
      "      (mlp): Mlp(\n",
      "        (fc1): Linear(in_features=96, out_features=192, bias=True)\n",
      "        (act): GELU(approximate='none')\n",
      "        (drop1): Dropout(p=0.0, inplace=False)\n",
      "        (norm): Identity()\n",
      "        (fc2): Linear(in_features=192, out_features=96, bias=True)\n",
      "        (drop2): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ls2): LayerScale()\n",
      "      (drop_path2): Identity()\n",
      "    )\n",
      "    (7): Block(\n",
      "      (norm1): LayerNorm((96,), eps=1e-06, elementwise_affine=True)\n",
      "      (attn): Attention(\n",
      "        (qkv): Linear(in_features=96, out_features=288, bias=True)\n",
      "        (q_norm): Identity()\n",
      "        (k_norm): Identity()\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(in_features=96, out_features=96, bias=True)\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ls1): LayerScale()\n",
      "      (drop_path1): Identity()\n",
      "      (norm2): LayerNorm((96,), eps=1e-06, elementwise_affine=True)\n",
      "      (mlp): Mlp(\n",
      "        (fc1): Linear(in_features=96, out_features=192, bias=True)\n",
      "        (act): GELU(approximate='none')\n",
      "        (drop1): Dropout(p=0.0, inplace=False)\n",
      "        (norm): Identity()\n",
      "        (fc2): Linear(in_features=192, out_features=96, bias=True)\n",
      "        (drop2): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ls2): LayerScale()\n",
      "      (drop_path2): Identity()\n",
      "    )\n",
      "    (8): Block(\n",
      "      (norm1): LayerNorm((96,), eps=1e-06, elementwise_affine=True)\n",
      "      (attn): Attention(\n",
      "        (qkv): Linear(in_features=96, out_features=288, bias=True)\n",
      "        (q_norm): Identity()\n",
      "        (k_norm): Identity()\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(in_features=96, out_features=96, bias=True)\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ls1): LayerScale()\n",
      "      (drop_path1): Identity()\n",
      "      (norm2): LayerNorm((96,), eps=1e-06, elementwise_affine=True)\n",
      "      (mlp): Mlp(\n",
      "        (fc1): Linear(in_features=96, out_features=192, bias=True)\n",
      "        (act): GELU(approximate='none')\n",
      "        (drop1): Dropout(p=0.0, inplace=False)\n",
      "        (norm): Identity()\n",
      "        (fc2): Linear(in_features=192, out_features=96, bias=True)\n",
      "        (drop2): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ls2): LayerScale()\n",
      "      (drop_path2): Identity()\n",
      "    )\n",
      "  )\n",
      "  (norm): LayerNorm((96,), eps=1e-06, elementwise_affine=True)\n",
      "  (attn_pool): AttentionPoolLatent(\n",
      "    (q): Linear(in_features=96, out_features=96, bias=True)\n",
      "    (kv): Linear(in_features=96, out_features=192, bias=True)\n",
      "    (q_norm): Identity()\n",
      "    (k_norm): Identity()\n",
      "    (proj): Linear(in_features=96, out_features=96, bias=True)\n",
      "    (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "    (norm): LayerNorm((96,), eps=1e-06, elementwise_affine=True)\n",
      "    (mlp): Mlp(\n",
      "      (fc1): Linear(in_features=96, out_features=192, bias=True)\n",
      "      (act): GELU(approximate='none')\n",
      "      (drop1): Dropout(p=0.0, inplace=False)\n",
      "      (norm): Identity()\n",
      "      (fc2): Linear(in_features=192, out_features=96, bias=True)\n",
      "      (drop2): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "  )\n",
      "  (fc_norm): Identity()\n",
      "  (head_drop): Dropout(p=0.0, inplace=False)\n",
      "  (head): Linear(in_features=96, out_features=1000, bias=True)\n",
      ")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "930280"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Very small models\n",
    "model_name = \"test_vit3.r160_in1k\"\n",
    "#model_name = \"test_vit2.r160_in1k\"\n",
    "#model_name = \"test_vit.r160_in1k\"\n",
    "\n",
    "# Bigger models\n",
    "#model_name = \"vit_wee_patch16_reg1_gap_256.sbb_in1k\" \n",
    "#model_name = \"vit_medium_patch16_reg4_gap_256.sbb_in1k\"\n",
    "\n",
    "# Even bigger\n",
    "# See bigger than tiny from: https://huggingface.co/timm/convnext_tiny.fb_in1k\n",
    "# Or others at https://huggingface.co/timm/vit_wee_patch16_reg1_gap_256.sbb_in1k\n",
    "\n",
    "model = timm.create_model(model_name, pretrained=True)\n",
    "model.cuda() \n",
    "print(model)\n",
    "\n",
    "sum(p.numel() for p in model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "470df42a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(model, val_loader):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for input, target in tqdm(val_loader):\n",
    "            with amp_autocast():\n",
    "                output = model(input)\n",
    "                correct += (target == output.argmax(dim=1)).sum().item()\n",
    "                total += target.numel()\n",
    "    print(\"val acc\", correct / total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fb277102",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_size': (3, 160, 160), 'interpolation': 'bicubic', 'mean': (0.5, 0.5, 0.5), 'std': (0.5, 0.5, 0.5), 'crop_pct': 0.95, 'crop_mode': 'center'}\n",
      "torch.Size([128, 3, 160, 160]) torch.Size([128])\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
      "        2, 2, 2, 2, 2, 2, 2, 2], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "data_config = timm.data.resolve_data_config(model=model)\n",
    "\n",
    "print(data_config)\n",
    "\n",
    "val_dataset = timm.data.create_dataset(\n",
    "    name=\"imagenet\",\n",
    "    split=\"validation\",\n",
    "    root=IMAGENET_PATH\n",
    ")\n",
    "\n",
    "val_loader = timm.data.create_loader(\n",
    "    val_dataset,\n",
    "    input_size=data_config['input_size'],                                                 \n",
    "    batch_size=128,\n",
    "    use_prefetcher=True,                                                       \n",
    "    interpolation=data_config['interpolation'],                                           \n",
    "    mean=data_config['mean'],\n",
    "    std=data_config['std'],\n",
    "    num_workers=8,                                                             \n",
    "    crop_pct=data_config[\"crop_pct\"],\n",
    "    crop_mode=data_config['crop_mode'],                                                   \n",
    "    crop_border_pixels=False,  \n",
    "    pin_memory=True,\n",
    "    device=device,\n",
    ")\n",
    "\n",
    "for bx, by in val_loader:\n",
    "    print(bx.shape, by.shape)\n",
    "    print(by)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "868de487",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/usamec/miniconda3/envs/test/lib/python3.11/site-packages/torch/utils/data/dataloader.py:617: UserWarning: This DataLoader will create 12 worker processes in total. Our suggested max number of worker in current system is 8, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "train_dataset = timm.data.create_dataset(\n",
    "    name=\"imagenet\",\n",
    "    split=\"train\",\n",
    "    root=IMAGENET_PATH\n",
    ")\n",
    "\n",
    "# TODO: augmentations\n",
    "train_loader = timm.data.create_loader(\n",
    "    train_dataset,\n",
    "    input_size=data_config['input_size'],                                                 \n",
    "    batch_size=128,\n",
    "    use_prefetcher=True,                                                       \n",
    "    interpolation=\"random\",  \n",
    "    mean=data_config['mean'],\n",
    "    std=data_config['std'],\n",
    "    num_workers=12,                                                             \n",
    "    crop_pct=data_config[\"crop_pct\"],\n",
    "    crop_mode=data_config['crop_mode'],                                                   \n",
    "    crop_border_pixels=False,  \n",
    "    pin_memory=True,\n",
    "    device=device,\n",
    "    is_training=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f579152f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b1875fe5eb994a9daa92bb62e3a8c191",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/391 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val acc 0.56908\n"
     ]
    }
   ],
   "source": [
    "# Run validation on full model (reports number in range 0-1)\n",
    "\n",
    "validate(model, val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c8be7458",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for gathering input statistics for a layer\n",
    "def update_cov(m, i, o):\n",
    "    x = i[0].detach().flatten(0, -2).float()\n",
    "    with torch.autocast(device_type=\"cuda\", enabled=False):\n",
    "        m.XX.data += x.square().sum(dim=0)\n",
    "\n",
    "# Selection functions for picking which layers to compress\n",
    "# In general, we do not compress first and last layer\n",
    "# Name of the layer is n.n2 (where there are no dots in n2)\n",
    "# Actual layer is m\n",
    "select_all_linear = lambda n, n2, m: type(m) == nn.Linear and \"head\" not in n and \"head\" not in n2\n",
    "select_only_attn_proj = lambda n, n2, m: type(m) == nn.Linear and \"head\" not in n and \"head\" not in n2 and \"proj\" in n2\n",
    "\n",
    "\n",
    "def process_model(model_name=model_name, replace_filter=select_only_attn_proj, \n",
    "                  replace_fn=lambda x: x, finetune_epochs=2, finetune_maxlr=1e-5,\n",
    "                  wanda=True):\n",
    "    \"\"\"\n",
    "    :model_name: Name of model\n",
    "    :replace_filter: Which layers to replace\n",
    "    :replace_fn: Takes a layer, outputs new layer (or sequence of layers)\n",
    "    :finetune_epochs: How many epochs to use for finetuning\n",
    "    :finetune_maxlr: Maximal learning rate for finetuning\n",
    "    :wanda: Whether to collect wanda statistics (if False, norms of inputs will be one)\n",
    "    \"\"\"\n",
    "    model = timm.create_model(model_name, pretrained=True)\n",
    "    model.cuda()\n",
    "    \n",
    "    print(\"pars before\", sum(p.numel() for p in model.parameters()))\n",
    "    \n",
    "    # need to dump, because changing during iteration would make a mess\n",
    "    to_change = []\n",
    "    for n, m in model.named_modules():\n",
    "        for n2, m2 in m.named_modules():\n",
    "            if \".\" not in n2 and replace_filter(n, n2, m2) and len(n2) > 0:\n",
    "                to_change.append((m, n2, m2))\n",
    "                if wanda:\n",
    "                    m2.XX = torch.zeros((m2.weight.shape[1]), device=m2.weight.device)\n",
    "                    m2.register_forward_hook(update_cov)\n",
    "                else:\n",
    "                    m2.XX = torch.ones((m2.weight.shape[1]), device=m2.weight.device)\n",
    "                \n",
    "    \n",
    "    # Gather input stats\n",
    "    if wanda:\n",
    "        step = 0\n",
    "        for input, _ in train_loader:\n",
    "            with amp_autocast():\n",
    "                with torch.no_grad():\n",
    "                    model(input)\n",
    "            step += 1\n",
    "            if step == 50:\n",
    "                break\n",
    "    \n",
    "    for m, n2, m2 in to_change:\n",
    "        setattr(m, n2, replace_fn(m2))\n",
    "    \n",
    "    print(\"pars after\", sum(p.numel() for p in model.parameters()))\n",
    "    print(\"first validation\")\n",
    "    validate(model, val_loader)\n",
    "    \n",
    "    if finetune_epochs == 0:\n",
    "        return\n",
    "    \n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=finetune_maxlr, weight_decay=1e-4)\n",
    "    scheduler = torch.optim.lr_scheduler.PolynomialLR(optimizer, total_iters=len(train_loader)*finetune_epochs)\n",
    "    scaler = torch.GradScaler(\"cuda\")\n",
    "\n",
    "    for ep in range(finetune_epochs):\n",
    "        model.train()\n",
    "        loss_sum = 0\n",
    "        loss_cc = 0\n",
    "        for input, target in (pbar := tqdm(train_loader)):\n",
    "            with amp_autocast():\n",
    "                output = model(input)\n",
    "                loss = F.cross_entropy(output, target)\n",
    "                scaler.scale(loss).backward()\n",
    "                scaler.step(optimizer)\n",
    "                scaler.update()\n",
    "                scheduler.step()\n",
    "                optimizer.zero_grad()\n",
    "                loss_sum += loss.item()\n",
    "                loss_cc += 1\n",
    "                pbar.set_description(\"loss: %.3f %.3f\" % (loss.item(), loss_sum / loss_cc))\n",
    "\n",
    "        validate(model, val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1db2c103",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pars before 930280\n",
      "auto rank 6\n",
      "Linear(in_features=96, out_features=96, bias=True) tensor(0.3823, device='cuda:0') tensor(3.6373, device='cuda:0')\n",
      "density 0.5026041666666666\n",
      "err 0.0008873476181179285 triv err 0.00307566300034523\n",
      "err norm 0.0007022766512818635 triv err 0.0034382150042802095\n",
      "auto rank 6\n",
      "Linear(in_features=96, out_features=96, bias=True) tensor(0.3820, device='cuda:0') tensor(2.1747, device='cuda:0')\n",
      "density 0.5026041666666666\n",
      "err 0.0007720351568423212 triv err 0.002187454840168357\n",
      "err norm 0.0005869496962986887 triv err 0.00246794824488461\n",
      "auto rank 6\n",
      "Linear(in_features=96, out_features=96, bias=True) tensor(0.3452, device='cuda:0') tensor(2.2589, device='cuda:0')\n",
      "density 0.5026041666666666\n",
      "err 0.0009386584861204028 triv err 0.0021895181853324175\n",
      "err norm 0.0006904646870680153 triv err 0.0023023795802146196\n",
      "auto rank 6\n",
      "Linear(in_features=96, out_features=96, bias=True) tensor(0.5048, device='cuda:0') tensor(2.6526, device='cuda:0')\n",
      "density 0.5026041666666666\n",
      "err 0.0009713922045193613 triv err 0.002281213877722621\n",
      "err norm 0.0007031977293081582 triv err 0.002346576424315572\n",
      "auto rank 6\n",
      "Linear(in_features=96, out_features=96, bias=True) tensor(0.5396, device='cuda:0') tensor(3.2310, device='cuda:0')\n",
      "density 0.5026041666666666\n",
      "err 0.0009829432237893343 triv err 0.0023565867450088263\n",
      "err norm 0.0007662802818231285 triv err 0.002530680038034916\n",
      "auto rank 6\n",
      "Linear(in_features=96, out_features=96, bias=True) tensor(0.5194, device='cuda:0') tensor(2.9634, device='cuda:0')\n",
      "density 0.5026041666666666\n",
      "err 0.0008821174851618707 triv err 0.0021126812789589167\n",
      "err norm 0.0006857517291791737 triv err 0.002063713502138853\n",
      "auto rank 6\n",
      "Linear(in_features=96, out_features=96, bias=True) tensor(0.6569, device='cuda:0') tensor(2.5072, device='cuda:0')\n",
      "density 0.5026041666666666\n",
      "err 0.0008377202320843935 triv err 0.00197451189160347\n",
      "err norm 0.0007059338968247175 triv err 0.0020842638332396746\n",
      "auto rank 6\n",
      "Linear(in_features=96, out_features=96, bias=True) tensor(0.7364, device='cuda:0') tensor(1.7805, device='cuda:0')\n",
      "density 0.5026041666666666\n",
      "err 0.0008115865057334304 triv err 0.0019406737992540002\n",
      "err norm 0.0007549155852757394 triv err 0.0019778641872107983\n",
      "auto rank 6\n",
      "Linear(in_features=96, out_features=96, bias=True) tensor(0.7538, device='cuda:0') tensor(1.2763, device='cuda:0')\n",
      "density 0.5026041666666666\n",
      "err 0.0008318668697029352 triv err 0.0020164772868156433\n",
      "err norm 0.0008063241839408875 triv err 0.0020349801052361727\n",
      "auto rank 6\n",
      "Linear(in_features=96, out_features=96, bias=True) tensor(0.7904, device='cuda:0') tensor(1.8733, device='cuda:0')\n",
      "density 0.5026041666666666\n",
      "err 0.0028212754987180233 triv err 0.006477300077676773\n",
      "err norm 0.002632597927004099 triv err 0.006421281490474939\n",
      "pars after 884200\n",
      "first validation\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ea0244cb51fe4495ab1d6f5e0f76c761",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/391 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val acc 0.10512\n"
     ]
    }
   ],
   "source": [
    "class BlockLinear2(nn.Module):\n",
    "    def __init__(self, nb=4, bs=(128,512), bias=False, dbg=False):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.weight = nn.Parameter(torch.zeros(nb, bs[0], bs[1]))\n",
    "        self.bias = None\n",
    "        if bias:\n",
    "            self.bias = nn.Parameter(torch.zeros(bs[1]))\n",
    "    \n",
    "    def extra_repr(self):\n",
    "        return \"w_shape %s\" % str(self.weight.shape)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        orig_shape = x.shape\n",
    "        x = x.flatten(0, -2)\n",
    "        x = x.reshape(x.shape[0], self.weight.shape[0], -1).permute(1, 0, 2)\n",
    "        x = x.matmul(self.weight)\n",
    "        x = x.permute(1, 0, 2)\n",
    "        x = x.reshape(tuple(list(orig_shape[0:-1]) + [-1]))\n",
    "        if self.bias is not None:\n",
    "            x = x + self.bias\n",
    "        return x\n",
    "\n",
    "class MonarchPerm2(nn.Module):\n",
    "    def __init__(self, nb=4, size=128):\n",
    "        super().__init__()\n",
    "        out = []\n",
    "        perm_base = torch.arange(size)\n",
    "        parts = perm_base.chunk(nb*nb)\n",
    "        for i in range(nb):\n",
    "            out += parts[i::nb]\n",
    "        self.perm = torch.cat(out).cuda()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        orig_shape = x.shape\n",
    "        x = x.flatten(0, -2)\n",
    "        out = x.T[self.perm].T\n",
    "        return out.reshape(orig_shape)\n",
    "    \n",
    "class LambdaLayer(nn.Module):\n",
    "    def __init__(self, f):\n",
    "        super().__init__()\n",
    "        self.f = f\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.f(x)\n",
    "\n",
    "def replace_with_monarch(m, n_blocks=4, rank=None):\n",
    "    assert m.weight.shape[0] % n_blocks == 0\n",
    "    assert m.weight.shape[1] % n_blocks == 0\n",
    "    if rank == None:\n",
    "        # ~50% of original weights\n",
    "        bs = m.weight.shape[0] // n_blocks, m.weight.shape[1] // n_blocks\n",
    "        rank = int((bs[0] * bs[1]) / (bs[0] + bs[1]) / 2)\n",
    "        print(\"auto rank\", rank)\n",
    "    \n",
    "    norm = m.XX.sqrt() + 1e-8\n",
    "    norm = norm / norm.mean()\n",
    "    print(m, norm.amin(), norm.amax())\n",
    "    W = m.weight.detach() * norm\n",
    "    mid = rank * n_blocks\n",
    "        \n",
    "    layer2 = nn.Sequential(\n",
    "        BlockLinear2(n_blocks, (W.shape[1]//n_blocks, mid)), \n",
    "        MonarchPerm2(n_blocks, mid*n_blocks),\n",
    "        BlockLinear2(n_blocks, (mid, W.shape[0]//n_blocks), bias=(m.bias is not None))\n",
    "    ).cuda()\n",
    "    \n",
    "    print(\"density\", sum(p.numel() for p in layer2.parameters()) / W.numel())\n",
    "\n",
    "    s0 = W.shape[0] // n_blocks\n",
    "    s1 = W.shape[1] // n_blocks\n",
    "    svd_err = 0\n",
    "    for i in range(n_blocks):\n",
    "        for j in range(n_blocks):\n",
    "            part = W[i*s0:i*s0+s0, j*s1:j*s1+s1]\n",
    "\n",
    "            U, s, Vh = torch.linalg.svd(part, full_matrices=False)\n",
    "            s = s[:rank]\n",
    "            U = U[:,:rank] * s.sqrt()\n",
    "            Vh = Vh[:rank] * s.sqrt().unsqueeze(1)\n",
    "            svd_err += (part - (U @ Vh)).square().sum().item()\n",
    "\n",
    "            assert layer2[0].weight.data[j, :,rank*i:rank*i+rank].numel() > 0\n",
    "            assert layer2[2].weight.data[i, rank*j:rank*j+rank,:].numel() > 0\n",
    "            layer2[0].weight.data[j, :, rank*i:rank*i+rank] = (Vh / norm[j*s1:j*s1+s1]).T\n",
    "            layer2[2].weight.data[i, rank*j:rank*j+rank] = U.T\n",
    "\n",
    "    \n",
    "    \n",
    "    if m.bias is not None:\n",
    "        layer2[2].bias.data = m.bias\n",
    "    \n",
    "    test = torch.eye(m.weight.shape[1], device=\"cuda\")\n",
    "    if m.bias is not None:\n",
    "        print(\"err\", (layer2(test) - m(test)).square().mean().item(), \"triv err\", (m(test) - m.bias.detach()).square().mean().item())\n",
    "    else:\n",
    "        print(\"err\", (layer2(test) - m(test)).square().mean().item(), \"triv err\", (m(test)).square().mean().item())\n",
    "    test = torch.eye(m.weight.shape[1], device=\"cuda\") * norm\n",
    "    if m.bias is not None:\n",
    "        print(\"err norm\", (layer2(test) - m(test)).square().mean().item(), \"triv err\", (m(test) - m.bias.detach()).square().mean().item())\n",
    "    else:\n",
    "        print(\"err norm\", (layer2(test) - m(test)).square().mean().item(), \"triv err\", (m(test)).square().mean().item())\n",
    "    \n",
    "    return layer2\n",
    "\n",
    "process_model(replace_fn=replace_with_monarch, finetune_epochs=0, finetune_maxlr=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1becce4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pars before 930280\n",
      "auto rank 6\n",
      "Linear(in_features=96, out_features=96, bias=True) tensor(1., device='cuda:0') tensor(1., device='cuda:0')\n",
      "density 0.5026041666666666\n",
      "err 0.0008055444923229516 triv err 0.00307566300034523\n",
      "err norm 0.0008055444923229516 triv err 0.00307566300034523\n",
      "auto rank 6\n",
      "Linear(in_features=96, out_features=96, bias=True) tensor(1., device='cuda:0') tensor(1., device='cuda:0')\n",
      "density 0.5026041666666666\n",
      "err 0.0006608118419535458 triv err 0.002187454840168357\n",
      "err norm 0.0006608118419535458 triv err 0.002187454840168357\n",
      "auto rank 6\n",
      "Linear(in_features=96, out_features=96, bias=True) tensor(1., device='cuda:0') tensor(1., device='cuda:0')\n",
      "density 0.5026041666666666\n",
      "err 0.0007813608972355723 triv err 0.0021895181853324175\n",
      "err norm 0.0007813608972355723 triv err 0.0021895181853324175\n",
      "auto rank 6\n",
      "Linear(in_features=96, out_features=96, bias=True) tensor(1., device='cuda:0') tensor(1., device='cuda:0')\n",
      "density 0.5026041666666666\n",
      "err 0.0008158438722603023 triv err 0.002281213877722621\n",
      "err norm 0.0008158438722603023 triv err 0.002281213877722621\n",
      "auto rank 6\n",
      "Linear(in_features=96, out_features=96, bias=True) tensor(1., device='cuda:0') tensor(1., device='cuda:0')\n",
      "density 0.5026041666666666\n",
      "err 0.0008840833907015622 triv err 0.0023565867450088263\n",
      "err norm 0.0008840833907015622 triv err 0.0023565867450088263\n",
      "auto rank 6\n",
      "Linear(in_features=96, out_features=96, bias=True) tensor(1., device='cuda:0') tensor(1., device='cuda:0')\n",
      "density 0.5026041666666666\n",
      "err 0.0007965319673530757 triv err 0.0021126812789589167\n",
      "err norm 0.0007965319673530757 triv err 0.0021126812789589167\n",
      "auto rank 6\n",
      "Linear(in_features=96, out_features=96, bias=True) tensor(1., device='cuda:0') tensor(1., device='cuda:0')\n",
      "density 0.5026041666666666\n",
      "err 0.0007748639327473938 triv err 0.00197451189160347\n",
      "err norm 0.0007748639327473938 triv err 0.00197451189160347\n",
      "auto rank 6\n",
      "Linear(in_features=96, out_features=96, bias=True) tensor(1., device='cuda:0') tensor(1., device='cuda:0')\n",
      "density 0.5026041666666666\n",
      "err 0.0007748798234388232 triv err 0.0019406737992540002\n",
      "err norm 0.0007748798234388232 triv err 0.0019406737992540002\n",
      "auto rank 6\n",
      "Linear(in_features=96, out_features=96, bias=True) tensor(1., device='cuda:0') tensor(1., device='cuda:0')\n",
      "density 0.5026041666666666\n",
      "err 0.000815429026260972 triv err 0.0020164772868156433\n",
      "err norm 0.000815429026260972 triv err 0.0020164772868156433\n",
      "auto rank 6\n",
      "Linear(in_features=96, out_features=96, bias=True) tensor(1., device='cuda:0') tensor(1., device='cuda:0')\n",
      "density 0.5026041666666666\n",
      "err 0.002689811633899808 triv err 0.006477300077676773\n",
      "err norm 0.002689811633899808 triv err 0.006477300077676773\n",
      "pars after 884200\n",
      "first validation\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2719950637e1433da43aded8c9d68c69",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/391 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val acc 0.03074\n"
     ]
    }
   ],
   "source": [
    "process_model(replace_fn=replace_with_monarch, finetune_epochs=0, finetune_maxlr=1e-4, wanda=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "33852696",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pars before 930280\n",
      "auto rank 6\n",
      "Linear(in_features=96, out_features=96, bias=True) tensor(0.3776, device='cuda:0') tensor(3.6518, device='cuda:0')\n",
      "density 0.5026041666666666\n",
      "err 0.0008888047304935753 triv err 0.00307566300034523\n",
      "err norm 0.0007026286912150681 triv err 0.0034289523027837276\n",
      "auto rank 6\n",
      "Linear(in_features=96, out_features=96, bias=True) tensor(0.3807, device='cuda:0') tensor(2.1671, device='cuda:0')\n",
      "density 0.5026041666666666\n",
      "err 0.0007724976167082787 triv err 0.002187454840168357\n",
      "err norm 0.0005873910849913955 triv err 0.0024656588211655617\n",
      "auto rank 6\n",
      "Linear(in_features=96, out_features=96, bias=True) tensor(0.3455, device='cuda:0') tensor(2.2605, device='cuda:0')\n",
      "density 0.5026041666666666\n",
      "err 0.0009388193138875067 triv err 0.0021895181853324175\n",
      "err norm 0.0006902138120494783 triv err 0.002301169093698263\n",
      "auto rank 6\n",
      "Linear(in_features=96, out_features=96, bias=True) tensor(0.5165, device='cuda:0') tensor(2.6154, device='cuda:0')\n",
      "density 0.5026041666666666\n",
      "err 0.0009699062211439013 triv err 0.002281213877722621\n",
      "err norm 0.000703853031154722 triv err 0.0023439510259777308\n",
      "auto rank 6\n",
      "Linear(in_features=96, out_features=96, bias=True) tensor(0.5471, device='cuda:0') tensor(3.2105, device='cuda:0')\n",
      "density 0.5026041666666666\n",
      "err 0.0009835693053901196 triv err 0.0023565867450088263\n",
      "err norm 0.0007663632277399302 triv err 0.00253018899820745\n",
      "auto rank 6\n",
      "Linear(in_features=96, out_features=96, bias=True) tensor(0.5144, device='cuda:0') tensor(2.9565, device='cuda:0')\n",
      "density 0.5026041666666666\n",
      "err 0.0008824851829558611 triv err 0.0021126812789589167\n",
      "err norm 0.0006858555716462433 triv err 0.002063166815787554\n",
      "auto rank 6\n",
      "Linear(in_features=96, out_features=96, bias=True) tensor(0.6522, device='cuda:0') tensor(2.5238, device='cuda:0')\n",
      "density 0.5026041666666666\n",
      "err 0.0008374734316021204 triv err 0.00197451189160347\n",
      "err norm 0.0007061132928356528 triv err 0.0020836053881794214\n",
      "auto rank 6\n",
      "Linear(in_features=96, out_features=96, bias=True) tensor(0.7374, device='cuda:0') tensor(1.7833, device='cuda:0')\n",
      "density 0.5026041666666666\n",
      "err 0.0008117036777548492 triv err 0.0019406737992540002\n",
      "err norm 0.0007548123248852789 triv err 0.0019780625589191914\n",
      "auto rank 6\n",
      "Linear(in_features=96, out_features=96, bias=True) tensor(0.7626, device='cuda:0') tensor(1.2659, device='cuda:0')\n",
      "density 0.5026041666666666\n",
      "err 0.0008321098866872489 triv err 0.0020164772868156433\n",
      "err norm 0.0008064431603997946 triv err 0.002035445999354124\n",
      "auto rank 6\n",
      "Linear(in_features=96, out_features=96, bias=True) tensor(0.7987, device='cuda:0') tensor(1.8970, device='cuda:0')\n",
      "density 0.5026041666666666\n",
      "err 0.0028259018436074257 triv err 0.006477300077676773\n",
      "err norm 0.002628639107570052 triv err 0.006419627461582422\n",
      "pars after 884200\n",
      "first validation\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9da6eaf37bbc44d99351ea0426011744",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/391 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val acc 0.10654\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "951cdf44a1864a218729ca8fa1c841e4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10009 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/usamec/miniconda3/envs/test/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:224: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "54238d9d77844bbba204b2852167bb52",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/391 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val acc 0.49554\n"
     ]
    }
   ],
   "source": [
    "# Toto trva dlho\n",
    "# process_model(replace_fn=replace_with_monarch, finetune_epochs=1, finetune_maxlr=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "141c9079",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
